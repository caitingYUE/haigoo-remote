# 业务逻辑文档 (Business Logic Documentation)

本文档整理了 Haigoo Assistant 项目的核心业务逻辑，特别是职位分类、数据源管理及同步机制。旨在方便后续开发与维护。

## 1. 职位区域分类逻辑 (Region Classification)

为了响应最新的业务调整（专注于国内岗位），系统已将重心转移至“国内可申”岗位。虽然数据库底层仍支持多区域分类，但在前端展示和人工审核环节，已实施严格的区域限制。

### 1.1 业务变更说明 (Recent Changes)
*   **旧逻辑**: 支持“国内可申”和“海外可申”双向展示。
*   **新逻辑**: **只收录和展示“国内可申”岗位**。
    *   **前端展示**: 仅展示国内（Domestic）及全球/远程（Global/Remote）岗位。
    *   **人工审核**: 仅通过符合“国内可申”标准的岗位。
    *   **后台数据**: 爬虫和 RSS 可能仍会抓取到海外岗位，但在人工审核阶段会被拦截或在前端被过滤。

### 1.2 区域分类定义 (Region Definitions)
数据库 `jobs` 表的 `region` 字段包含以下三种状态：

1.  **`domestic` (纯国内)**
    *   **定义**: 仅限中国大陆及港澳台地区的线下岗位。
    *   **关键词**: China, CN, Beijing, Shanghai, Shenzhen, Hong Kong, Taiwan, Macau 等。
    *   **适用场景**: **核心展示对象**。

2.  **`global` (全球/远程/亚太)**
    *   **定义**: 不限地点、全球远程、亚太区（APAC）或时区友好的岗位。此类岗位适合国内候选人申请（远程/亚太）。
    *   **关键词**: Anywhere, Global, Remote, APAC, Asia Pacific, GMT+8, UTC+8 等。
    *   **适用场景**: **核心展示对象**（作为国内可申的一部分）。

3.  **`overseas` (纯海外)**
    *   **定义**: 明确为欧美、日韩、东南亚等非大中华区的线下岗位。
    *   **关键词**: USA, UK, Europe, Japan, Singapore, Australia, North America 等。
    *   **适用场景**: **不再主动展示**（仅在后台保留数据，供未来扩展或内部参考）。

### 1.3 筛选与展示逻辑 (Filtering Logic)
在后端接口 (`lib/api-handlers/processed-jobs.js`) 及前端查询中，默认应用以下过滤规则：

*   **默认查询**:
    *   SQL: `region IN ('domestic', 'global')`
    *   结果: 仅返回国内岗位 + Global/APAC 岗位。
*   **海外岗位处理**:
    *   `overseas` 类型的岗位虽然可能存在于数据库中，但默认不会返回给 C 端用户，除非特定管理接口查询。

---

## 2. 数据源与可信状态 (Source Type & Trusted Status)

为了区分岗位的来源可靠性和推荐优先级，系统使用了 `source_type` 和 `is_trusted` 两个字段的组合逻辑。

### 2.1 状态映射表

| 来源类型 | 对应字段值 | 业务含义 | 标识 |
| :--- | :--- | :--- | :--- |
| **企业官网直投** | `source_type='official'` <br> `is_trusted=true` | 来自【可信企业管理】列表中的企业，通过爬虫直接抓取官网。质量最高，无中间商。 | 橙色认证徽章 (Official) |
| **精选平台/社区** | `source_type='trusted'` <br> `is_trusted=false` | 来自人工运营的精选 RSS 源（如特定社区、招聘聚合站）。经过人工筛选，但非官网直投。 | 蓝色/无徽章 (Trusted Platform) |
| **第三方/普通RSS** | `source_type='rss'` <br> `is_trusted=false` | 来自广泛的第三方 RSS 源。未经人工严格筛选，仅做基础聚合。 | 无徽章 (Third-party) |
| **内推** | `can_refer=true` | 带有内推属性的岗位，优先级最高。 | 专属内推标识 |

### 2.2 数据清洗规则

*   若岗位关联了 `trusted_companies` 表中的企业，**强制**更新为 `source_type='official'` 且 `is_trusted=true`。
*   若 `source_type` 为 `trusted`，则 `is_trusted` 必须为 `false`（避免混淆官网直投和精选平台）。

---

## 3. 企业数据管理 (Company Data Management)

系统中的企业数据分为【可信企业】和【全部企业】两类，二者通过不同的维护方式和数据流向进行管理。

### 3.1 可信企业 (Trusted Companies)
*   **来源**: 由管理员在后台人工录入、审查。
*   **用途**: 作为高质量数据源的基准，关联“官网直投”岗位。
*   **数据流向**: 可信企业的信息（Logo、简介、行业等）拥有最高优先级，会强制同步覆盖关联岗位的信息。

### 3.2 全部企业 (All Companies)
*   **来源**: 综合了【可信企业】和【三方RSS聚合】的所有企业数据。
*   **维护机制**:
    *   **刷新数据 (Extract)**: 通过“刷新数据”操作，系统扫描 `jobs` 表，提取所有出现的 unique 企业名称，构建初步的企业列表。
    *   **智能补全 (Enrichment)**:
        *   **自动补全并翻译**: 设计爬虫算法抓取企业官网，获取 Logo、简介等信息。
        *   **AI 分析标签**: 基于企业简介，利用 AI 分析出企业的行业归属（Industry）和特性标签（Tags）。
    *   **反向同步 (Sync Back)**: 经过清洗和补全的企业信息（行业、标签、简介），会通过 `sync-jobs` 机制反向更新到 `jobs` 表中，确保职位列表显示的行业和标签准确。

---

## 4. 可信企业爬虫逻辑 (Trusted Company Crawler Logic)

针对【可信企业】的岗位抓取，系统设计了严格的更新、保护和防误删机制，以确保数据的新鲜度同时保留人工运营成果。

### 4.1 核心更新策略 (Sync Strategy)
*   **触发**: 手动触发（API）或定时任务触发（Cron Job）。
*   **统一入口**: 所有更新均调用 `job-sync-service.js` 的 `syncCompanyJobs` 服务，确保逻辑一致。

### 4.2 智能同步与模糊匹配 (Smart Fuzzy Matching)
爬虫不再简单粗暴地执行“先删后插”，而是采用更智能的对比逻辑：

1.  **直接匹配 (Direct Match)**:
    *   **依据**: URL 或 ID 完全一致。
    *   **操作**: 视为**更新 (Update)**。
    *   **字段保留规则 (Field Preservation)**:
        *   **管理标记 (Administrative Flags)**: **强制保留**。包括精选 (`is_featured`)、审核状态 (`is_approved`)、风险评级 (`risk_rating`)、内部备注 (`haigoo_comment`)、隐藏字段 (`hidden_fields`)、可内推状态 (`can_refer`, 若旧值为真)。
        *   **内容字段 (Content Fields)**: 仅当 `is_manually_edited=true` 或内容高度相似 (>80%) 时保留。否则会被新爬取内容覆盖。

2.  **模糊匹配 (Fuzzy Match)**:
    *   **背景**: 企业官网改版常导致岗位 URL 变更，旧逻辑会将其视为“旧岗位删除 + 新岗位新增”，导致人工数据丢失。
    *   **依据**: 标题 (Title) 完全一致 + 描述 (JD) 相似度 > 80% (基于 Jaccard 算法，优化了技术栈关键词分词)。
    *   **操作**: 视为**迁移 (Migrate)**。
    *   **机制**:
        *   **复用 ID**: 将新抓取岗位的 ID 强制设为旧岗位的 ID。
        *   **原地更新**: 执行 Update 操作而非 Delete + Insert。
        *   **全量继承**: 直接继承旧岗位的所有管理标记和描述内容，防止人工运营成果丢失。
        *   **效果**: 用户的收藏、申请记录、分享链接全部保持有效，且人工编辑内容得以保留。

3.  **新增 (New)**:
    *   **依据**: 既无直接匹配，也无模糊匹配的新数据。
    *   **操作**: 执行 **插入 (Insert)**。
    *   **默认状态**: `is_approved=false` (待审核), `is_featured=false`。

4.  **删除 (Delete)**:
    *   **依据**: 数据库中存在但爬虫未抓取到，且未匹配到任何新抓取数据的旧岗位。
    *   **操作**: 视为**失效 (Obsolete)**，执行删除。

5.  **RSS/第三方覆盖规则 (RSS Overwrite Rules)**:
    *   **逻辑**: RSS 数据流通常采用“全量覆盖”逻辑。如果 RSS 源推送了相同 ID 的数据更新，**所有字段（包括精选状态）可能会被覆盖**。
    *   **建议**: 仅对 RSS 数据做基础筛选，精细化运营建议聚焦于【可信企业】数据源。

### 4.3 安全保护机制 (Safety Mechanisms)
为了防止意外情况导致数据灾难，系统内置了两重保护：

1.  **人工数据绝对保护**:
    *   **规则**: 即使判定为“失效”，如果岗位的 `source='manual'`（纯手工录入），系统**绝不删除**，必须由管理员人工处理。

2.  **安全刹车 (Safety Brake)**:
    *   **背景**: 防止因爬虫故障（如页面解析失败、分页失效）导致只抓取到少量数据，从而误删大量正常岗位。
    *   **规则**: 如果该企业存量岗位 > 10，且本次同步试图删除超过 **50%** 的存量岗位。
    *   **动作**: 系统触发警告并**中止删除操作**（仅执行新增和更新），保护现有数据不被清空。

---

## 5. 数据清洗与维护 (Data Maintenance)

### 5.1 SQL 清洗脚本

位于 `scripts/data-cleaning/01_clean_jobs_data.sql`，用于修复历史数据污染和逻辑变更后的数据刷新。

**主要步骤**:
1.  **重置**: 将所有岗位的 `is_remote` 设为默认值。
2.  **Region 计算**: 依次执行 Global -> Domestic -> Overseas 的更新语句（注意顺序和排除逻辑）。
3.  **来源修复**: 基于 `trusted_companies` 关联修复 `source_type` 和 `is_trusted`。
4.  **字段同步**: 强制同步 `industry` 和 `tags`。

### 5.2 注意事项

*   **严禁 Mock**: 生产环境严禁使用 Mock 数据，必须基于真实数据库查询。
*   **数据库变更**: 修改表结构必须更新 `server-utils/dal/neon-ddl.sql`，并注明日期和原因。

---

## 6. 后台数据处理全流程 (Backend Data Processing Workflow)

本节详细描述从数据获取到最终展示的完整数据流转过程。

### 6.1 RSS 数据获取与处理
*   **来源**: 【RSS页面】
*   **操作**: 获取原始 RSS 数据 (Raw Data)。
*   **流程**:
    1.  **解析与理解**: 在【职位数据页面】，系统对 RSS 数据进行解析和语义理解。
    2.  **字段判定**: 处理并确定关键字段，包括：
        *   **基础信息**: 岗位分类、级别、企业名称、URL、岗位类型。
        *   **核心逻辑**: 区域限制、区域分类 (Region)、技能标签 (Tags)、语言要求、发布日期、岗位来源。
    3.  **翻译**: 执行岗位数据的翻译操作，生成中文对照。

### 6.2 企业信息聚合与补全
*   **来源**: 【企业管理】(All Companies)
*   **操作**:
    1.  **聚合**: 系统自动聚合职位数据中的企业信息（如 URL、岗位数量），构建【全部企业】列表。
    2.  **自动补全并翻译**: 点击按钮后，触发爬虫抓取企业官网，获取 Logo 和简介，并自动补充到当前页面。
    3.  **AI 分析标签**: 点击按钮后，后台基于企业简介，利用 AI 智能分析企业的所属行业 (Industry) 并打上特性标签 (Tags)。

### 6.3 可信企业管理
*   **来源**: 【可信企业管理】
*   **操作**:
    1.  **人工添加**: 支持管理员点击添加按钮手动录入企业。
    2.  **智能辅助**: 支持自动抓取企业简介、Logo 和岗位数据；支持基于简介判断行业、添加标签；支持判断是否可内推。
    3.  **数据归一**: 可信企业最终也会被归类到【全部企业】的大池子中，且其抓取的高质量岗位数据会同步更新到所有岗位数据中，供前端展示。

---

## 7. 定时任务架构 (Cron Job Architecture)

系统基于 Vercel Cron 实现自动化数据更新，核心任务分为三大类。

### 7.1 任务调度表 (Schedule)
| 任务名称 | API 路径 | 触发时间 (UTC) | 功能描述 |
| :--- | :--- | :--- | :--- |
| **Daily Ingest** | `/api/cron/daily-ingest` | 01:30 | **RSS抓取与入库**。串行执行 `stream-fetch-rss` (抓取) 和 `stream-process-rss` (处理)。 |
| **Daily Enrich** | `/api/cron/daily-enrich` | 02:00 | **数据翻译与补全**。串行执行 `stream-translate-jobs` (翻译) 和 `stream-enrich-companies` (补全企业信息)。 |
| **Trusted Crawl** | `/api/cron/stream-crawl-trusted-jobs` | 每4小时 | **可信企业官网爬取**。独立运行，高频更新可信源数据。 |
| **Rotate Featured** | `/api/cron/rotate-featured` | 03:00 | **精选职位轮换**。更新首页推荐位。 |

### 7.2 核心机制与优化 (Core Mechanisms)

#### A. 增量保存与分批处理 (Incremental Saving & Chunking)
针对 Serverless 环境的执行时长限制（通常 10-15秒或更长），所有耗时任务（如 RSS 抓取）均采用了**流式处理**模式：
*   **分批 (Chunking)**: 将大量 RSS 源按 3-5 个一组进行分批。
*   **即时入库 (Immediate Upsert)**: 每处理完一批数据，**立即**执行数据库 `INSERT ON CONFLICT UPDATE` 操作。
*   **优势**: 即使任务在第 N 批次超时被杀，前 N-1 批次的数据已安全入库，杜绝了“跑了很久最后入库失败导致 +0” 的问题。

#### B. 独立容错 (Isolated Error Handling)
*   每个 RSS 源或爬取任务的错误都被独立捕获。
*   单个源的解析失败、超时不会中断整个 Cron 任务流，确保其他源能正常更新。

#### C. 安全熔断 (Safety Circuit Breaker)
*   **RSS 处理**: 设置了 `MAX_BATCHES` (如 20批次/1000条)，防止因数据积压导致的无限循环和资源耗尽。
*   **可信爬虫**: 单个企业爬取设置独立超时限制，防止卡死。
